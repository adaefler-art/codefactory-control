name: Deploy AFU-9 to ECS

# ============================================================================
# CANONICAL DEPLOY GUARDRAILS (Single Source of Truth)
# - docs/deploy/AFU9_DEPLOY_SYSTEM_PROMPT.md
# - docs/deploy/AFU9_DEPLOY_CHECKLIST.md
# - docs/deploy/AFU9_DEPLOY_INTENT.md
#
# Workflow separation:
# - deploy-ecs.yml       => APP-ONLY (ECS task definition + service update)
# - deploy-cdk-stack.yml => INFRA (CDK deploy; DNS only if MANAGE_DNS=true)
#
# Do not widen IAM permissions or bypass preflight/diff gates.
# ============================================================================

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy to"
        required: true
        type: choice
        options:
          - production
          - staging
        default: production
      run_migrations:
        description: "Run database migrations (auto: detect by changed files)"
        required: false
        type: choice
        options:
          - auto
          - "true"
          - "false"
        default: auto
      run_alb_checks:
        description: "Run ALB DNS + target health checks"
        required: false
        type: choice
        options:
          - "true"
          - "false"
        default: "false"
  push:
    branches:
      - main
    paths:
      - "control-center/**"
      - "mcp-servers/**"
      - ".github/workflows/deploy-ecs.yml"

permissions:
  id-token: write
  contents: read

concurrency:
  group: deploy-ecs-${{ github.event_name == 'workflow_dispatch' && github.event.inputs.environment || 'staging' }}
  cancel-in-progress: false

env:
  AWS_REGION: eu-central-1
  DEPLOY_ENV: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.environment || 'staging' }}
  DOMAIN_NAME: afu-9.com
  MANAGE_DNS: ${{ vars.AFU9_MANAGE_DNS || 'false' }}
  AFU9_ENABLE_HTTPS: ${{ vars.AFU9_ENABLE_HTTPS || 'true' }}

jobs:
  build-and-deploy:
    name: Build and Deploy to ECS
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Decide gates
        id: gates
        shell: pwsh
        run: |
          $ErrorActionPreference = 'Stop'

          $eventName = "${{ github.event_name }}"
          $deployEnv = "${{ env.DEPLOY_ENV }}"

          # Defaults
          $runMigrationsInput = "${{ github.event.inputs.run_migrations }}"
          if (-not $runMigrationsInput) { $runMigrationsInput = 'auto' }

          $runAlbChecksInput = "${{ github.event.inputs.run_alb_checks }}"
          if (-not $runAlbChecksInput) { $runAlbChecksInput = 'false' }

          $shouldMigrate = $false
          $runAlbChecks = ($runAlbChecksInput -eq 'true')

          function Test-MigrationRelevant([string[]]$paths) {
            foreach ($p in ($paths | Where-Object { $_ })) {
              if ($p -match '(^|/)(migrations/|prisma/|supabase/|database/migrations/)' -or
                  $p -match '(^|/)(schema\.prisma|workflow-schema\.json)$') {
                return $true
              }
            }
            return $false
          }

          if ($eventName -eq 'workflow_dispatch') {
            # Respect inputs; auto => detect by recent commit diff (best-effort)
            if ($runMigrationsInput -eq 'true') {
              $shouldMigrate = $true
            } elseif ($runMigrationsInput -eq 'false') {
              $shouldMigrate = $false
            } else {
              $changed = @()
              try {
                $changed = git diff --name-only HEAD~1 HEAD 2>$null
              } catch {
                $changed = @()
              }
              $shouldMigrate = (Test-MigrationRelevant -paths $changed)
            }
          } else {
            # Push: auto-detect by changed files
            $before = "${{ github.event.before }}"
            $after = "${{ github.sha }}"
            $changed = @()
            try {
              if ($before -and $before -ne '0000000000000000000000000000000000000000') {
                $changed = git diff --name-only $before $after
              } else {
                $changed = git diff --name-only HEAD~1 HEAD
              }
            } catch {
              $changed = @()
            }
            $shouldMigrate = (Test-MigrationRelevant -paths $changed)
          }

          Write-Host "Decide gates: event=$eventName env=$deployEnv run_migrations_input=$runMigrationsInput => should_migrate=$shouldMigrate run_alb_checks=$runAlbChecks"

          "should_migrate=$($shouldMigrate.ToString().ToLowerInvariant())" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append
          "run_alb_checks=$($runAlbChecks.ToString().ToLowerInvariant())" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append

      - name: Display trigger context
        run: echo "Triggered by ${{ github.event_name }} on branch ${{ github.ref_name }} @ ${{ github.sha }}"

      - name: Guard production deploys must be from main
        shell: bash
        run: |
          set -euo pipefail
          if [ "${{ env.DEPLOY_ENV }}" = "production" ] && [ "${{ github.ref_name }}" != "main" ]; then
            echo "‚ùå Production deploys must run from main (ref=${{ github.ref_name }})" >&2
            exit 1
          fi

      - name: Guard AWS_DEPLOY_ROLE_ARN
        run: |
          if [ -z "${{ secrets.AWS_DEPLOY_ROLE_ARN }}" ]; then
            echo "‚ùå AWS_DEPLOY_ROLE_ARN is missing (secret is empty or undefined)" >&2
            exit 1
          fi

      - name: Resolve deploy target (guarded)
        id: target
        shell: pwsh
        run: |
          $deployEnv = "${{ env.DEPLOY_ENV }}"
          $stagingCluster = "${{ vars.STAGING_ECS_CLUSTER }}"
          if (-not $stagingCluster) { $stagingCluster = 'afu9-cluster' }

          if ($deployEnv -ne 'production' -and $deployEnv -ne 'staging') {
            Write-Error "Unsupported DEPLOY_ENV: $deployEnv"; exit 1
          }

          if ($deployEnv -eq 'production') {
            $tagPrefix = 'prod'
            $ecsService = 'afu9-control-center'
            $ecsCluster = 'afu9-cluster'
            $envLabel = 'Production'
            $readyHost = 'afu-9.com'
          } else {
            $tagPrefix = 'stage'
            $ecsService = 'afu9-control-center-staging'
            $ecsCluster = $stagingCluster
            $envLabel = 'Staging'
            $readyHost = 'stage.afu-9.com'
          }

          if ($deployEnv -eq 'staging' -and $ecsService -eq 'afu9-control-center') {
            Write-Error "Staging deployment would target production service (afu9-control-center)."; exit 1
          }
          if ($deployEnv -eq 'staging' -and ($ecsService -notmatch 'staging')) {
            Write-Error "Staging deployment must use a staging service name. Got: $ecsService"; exit 1
          }
          if ($deployEnv -eq 'production' -and ($ecsService -match 'staging')) {
            Write-Error "Production deployment must not target a staging service name. Got: $ecsService"; exit 1
          }

          "deploy_env=$deployEnv" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append
          "tag_prefix=$tagPrefix" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append
          "ecs_service=$ecsService" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append
          "env_label=$envLabel" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append
          "ecs_cluster=$ecsCluster" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append
          "ready_host=$readyHost" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - name: Install root dependencies
        shell: bash
        run: |
          set -euo pipefail
          npm ci

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_DEPLOY_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Deploy Context Guardrail (E7.0.1)
        shell: bash
        env:
          DEPLOY_ENV: ${{ steps.target.outputs.deploy_env }}
          ECS_SERVICE: ${{ steps.target.outputs.ecs_service }}
          ECS_CLUSTER: ${{ steps.target.outputs.ecs_cluster }}
          CREATE_STAGING_SERVICE: 'false'
          AFU9_ENABLE_HTTPS: ${{ env.AFU9_ENABLE_HTTPS }}
          MANAGE_DNS: ${{ env.MANAGE_DNS }}
        run: |
          set -euo pipefail
          echo "üîí Running Deploy Context Guardrail (E7.0.1)"
          echo "Environment: ${DEPLOY_ENV}"
          echo "Service: ${ECS_SERVICE}"
          echo "Cluster: ${ECS_CLUSTER}"
          echo ""
          
          # Run guardrail to validate environment context
          # This enforces fail-closed: no implicit prod, explicit DEPLOY_ENV required,
          # and cross-env artifact detection (stage/prod isolation).
          # Use npx with ts-node for deterministic execution
          npx ts-node scripts/deploy-context-guardrail.ts
          
          echo ""
          echo "‚úÖ Guardrail passed - proceeding with deploy"

      - name: Preflight gate
        shell: bash
        env:
          DEPLOY_ENV: ${{ steps.target.outputs.deploy_env }}
          AWS_REGION: ${{ env.AWS_REGION }}
          DOMAIN_NAME: ${{ env.DOMAIN_NAME }}
          MANAGE_DNS: ${{ env.MANAGE_DNS }}
          ECS_CLUSTER: ${{ steps.target.outputs.ecs_cluster }}
          ECS_SERVICE: ${{ steps.target.outputs.ecs_service }}
          STAGING_SERVICE: afu9-control-center-staging
          REQUIRE_STAGING_SERVICE: ${{ steps.target.outputs.deploy_env == 'staging' && 'true' || 'false' }}
          CREATE_STAGING_SERVICE: 'false'
          AFU9_ENABLE_HTTPS: ${{ env.AFU9_ENABLE_HTTPS }}
          PROTECTED_RESOURCES: ${{ env.MANAGE_DNS == 'true' && 'cluster,roles,listeners' || 'cluster,roles,listeners,route53' }}
          CDK_STACKS: Afu9EcsStack Afu9RoutingSingleEnvStack
        run: |
          set -euo pipefail
          echo "MANAGE_DNS=${MANAGE_DNS}"
          echo "DEPLOY_ENV=${DEPLOY_ENV} CLUSTER=${ECS_CLUSTER} SERVICE=${ECS_SERVICE}"
          echo "DOMAIN_NAME=${DOMAIN_NAME} CREATE_STAGING_SERVICE=${CREATE_STAGING_SERVICE}"
          chmod +x ./scripts/preflight.sh

          # Preflight diff must reflect the *currently deployed* infra toggles.
          # If we force HTTPS/DNS off in the diff args, CDK will propose deleting listeners/record sets
          # and the diff gate will (correctly) block the deploy.
          EFFECTIVE_ENABLE_HTTPS="${AFU9_ENABLE_HTTPS}"
          EFFECTIVE_MANAGE_DNS="${MANAGE_DNS}"

          if aws cloudformation describe-stacks --stack-name Afu9NetworkStack --region "${AWS_REGION}" >/dev/null 2>&1; then
            STACK_HTTPS=$(aws cloudformation describe-stacks \
              --stack-name Afu9NetworkStack \
              --region "${AWS_REGION}" \
              --query "Stacks[0].Outputs[?OutputKey=='HttpsEnabled'].OutputValue | [0]" \
              --output text 2>/dev/null || true)
            if [ "${STACK_HTTPS}" = "true" ] || [ "${STACK_HTTPS}" = "false" ]; then
              EFFECTIVE_ENABLE_HTTPS="${STACK_HTTPS}"
            fi

            DNS_RS_COUNT=$(aws cloudformation list-stack-resources \
              --stack-name Afu9NetworkStack \
              --region "${AWS_REGION}" \
              --query "StackResourceSummaries[?ResourceType=='AWS::Route53::RecordSet'] | length(@)" \
              --output text 2>/dev/null || true)
            if [ -n "${DNS_RS_COUNT:-}" ] && [ "${DNS_RS_COUNT}" != "None" ] && [ "${DNS_RS_COUNT}" -gt 0 ] 2>/dev/null; then
              EFFECTIVE_MANAGE_DNS="true"
            fi
          fi

          echo "[info] effective cdk diff toggles: afu9-enable-https=${EFFECTIVE_ENABLE_HTTPS} afu9-manage-dns=${EFFECTIVE_MANAGE_DNS}"

          # Build CDK diff arguments for the guarded stacks
          CDK_ARGS=(
            --app "npx ts-node --project tsconfig.json bin/codefactory-control.ts"
            -c afu9-multi-env=false
            -c afu9-domain="${DOMAIN_NAME}"
            -c afu9-enable-https="${EFFECTIVE_ENABLE_HTTPS}"
            -c afu9-manage-dns="${EFFECTIVE_MANAGE_DNS}"
            -c afu9-create-staging-service="${CREATE_STAGING_SERVICE}"
            -c environment="${DEPLOY_ENV}"
          )

          for stack in ${CDK_STACKS}; do
            CDK_ARGS+=("${stack}")
          done

          echo "[info] protected resources: ${PROTECTED_RESOURCES}"
          echo "[info] running preflight for cluster=${ECS_CLUSTER} service=${ECS_SERVICE} manageDns=${MANAGE_DNS}"
          echo "[info] cdk args: ${CDK_ARGS[*]}"

          # Encode args as JSON to preserve quoting (notably --app "npx ts-node ...").
          CDK_ARGS_JSON=$(printf '%s\n' "${CDK_ARGS[@]}" | jq -R . | jq -s .)

          ./scripts/preflight.sh \
            --cluster "${ECS_CLUSTER}" \
            --service "${ECS_SERVICE}" \
            --staging-service "${STAGING_SERVICE}" \
            --require-staging "${REQUIRE_STAGING_SERVICE}" \
            --manage-dns "${MANAGE_DNS}" \
            --create-staging-service "${CREATE_STAGING_SERVICE}" \
            --domain "${DOMAIN_NAME}" \
            --cdk-args-json "${CDK_ARGS_JSON}"

      - name: Install database client tooling
        shell: bash
        run: |
          set -euo pipefail
          sudo apt-get update
          sudo apt-get install -y postgresql-client jq


      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Get ECR repository URIs
        id: ecr-uris
        shell: bash
        run: |
          set -euo pipefail
          echo "control_center=$(aws ecr describe-repositories --repository-names afu9/control-center --query 'repositories[0].repositoryUri' --output text)" >> "$GITHUB_OUTPUT"
          echo "mcp_github=$(aws ecr describe-repositories --repository-names afu9/mcp-github --query 'repositories[0].repositoryUri' --output text)" >> "$GITHUB_OUTPUT"
          echo "mcp_deploy=$(aws ecr describe-repositories --repository-names afu9/mcp-deploy --query 'repositories[0].repositoryUri' --output text)" >> "$GITHUB_OUTPUT"
          echo "mcp_observability=$(aws ecr describe-repositories --repository-names afu9/mcp-observability --query 'repositories[0].repositoryUri' --output text)" >> "$GITHUB_OUTPUT"
          echo "mcp_runner=$(aws ecr describe-repositories --repository-names afu9/mcp-runner --query 'repositories[0].repositoryUri' --output text)" >> "$GITHUB_OUTPUT"

      - name: Generate image tags
        id: image-tags
        shell: bash
        run: |
          set -euo pipefail
          SHORT_SHA=$(echo "${{ github.sha }}" | cut -c1-7)
          echo "short_sha=${SHORT_SHA}" >> "$GITHUB_OUTPUT"
          echo "full_sha=${{ github.sha }}" >> "$GITHUB_OUTPUT"

      - name: Generate build metadata
        id: build-meta
        shell: bash
        run: |
          set -euo pipefail
          VERSION=$(node -p "require('./control-center/package.json').version")
          TS=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          echo "version=$VERSION" >> "$GITHUB_OUTPUT"
          echo "timestamp=$TS" >> "$GITHUB_OUTPUT"

      - name: Build and push Control Center
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./control-center/Dockerfile
          push: true
          build-args: |
            BUILD_VERSION=${{ steps.build-meta.outputs.version }}
            BUILD_COMMIT_HASH=${{ github.sha }}
            BUILD_ENV=${{ steps.target.outputs.deploy_env }}
            BUILD_TIMESTAMP=${{ steps.build-meta.outputs.timestamp }}
          tags: |
              ${{ steps.ecr-uris.outputs.control_center }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.short_sha }}
              ${{ steps.ecr-uris.outputs.control_center }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.full_sha }}
              ${{ steps.ecr-uris.outputs.control_center }}:${{ steps.target.outputs.tag_prefix }}-latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push MCP GitHub Server
        uses: docker/build-push-action@v5
        with:
          context: ./mcp-servers
          file: ./mcp-servers/github/Dockerfile
          push: true
          tags: |
            ${{ steps.ecr-uris.outputs.mcp_github }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.short_sha }}
            ${{ steps.ecr-uris.outputs.mcp_github }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.full_sha }}
            ${{ steps.ecr-uris.outputs.mcp_github }}:${{ steps.target.outputs.tag_prefix }}-latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push MCP Deploy Server
        uses: docker/build-push-action@v5
        with:
          context: ./mcp-servers
          file: ./mcp-servers/deploy/Dockerfile
          push: true
          tags: |
            ${{ steps.ecr-uris.outputs.mcp_deploy }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.short_sha }}
            ${{ steps.ecr-uris.outputs.mcp_deploy }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.full_sha }}
            ${{ steps.ecr-uris.outputs.mcp_deploy }}:${{ steps.target.outputs.tag_prefix }}-latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push MCP Observability Server
        uses: docker/build-push-action@v5
        with:
          context: ./mcp-servers
          file: ./mcp-servers/observability/Dockerfile
          push: true
          tags: |
            ${{ steps.ecr-uris.outputs.mcp_observability }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.short_sha }}
            ${{ steps.ecr-uris.outputs.mcp_observability }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.full_sha }}
            ${{ steps.ecr-uris.outputs.mcp_observability }}:${{ steps.target.outputs.tag_prefix }}-latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push MCP Runner Server
        uses: docker/build-push-action@v5
        with:
          context: ./mcp-servers
          file: ./.github/docker/mcp-runner.Dockerfile
          push: true
          tags: |
            ${{ steps.ecr-uris.outputs.mcp_runner }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.short_sha }}
            ${{ steps.ecr-uris.outputs.mcp_runner }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.full_sha }}
            ${{ steps.ecr-uris.outputs.mcp_runner }}:${{ steps.target.outputs.tag_prefix }}-latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Deploy Provenance (hard gate)
        id: provenance
        shell: bash
        run: |
          set -euo pipefail
          BRANCH="${{ github.ref_name }}"
          SHA="${{ github.sha }}"
          SHORT_SHA="${{ steps.image-tags.outputs.short_sha }}"
          FULL_SHA="${{ steps.image-tags.outputs.full_sha }}"
          DEPLOY_ENV="${{ steps.target.outputs.deploy_env }}"
          TAG_PREFIX="${{ steps.target.outputs.tag_prefix }}"
          CLUSTER="${{ steps.target.outputs.ecs_cluster }}"
          SERVICE="${{ steps.target.outputs.ecs_service }}"
          REPO_URI="${{ steps.ecr-uris.outputs.control_center }}"
          IMAGE_TAG="${TAG_PREFIX}-${FULL_SHA}"

          echo "Branch: ${BRANCH}"
          echo "SHA: ${SHA}"
          echo "Deploy env: ${DEPLOY_ENV}"
          echo "Target: cluster=${CLUSTER} service=${SERVICE}"
          echo "Image: ${REPO_URI}:${IMAGE_TAG}"

          if [ "${DEPLOY_ENV}" != "staging" ] && [ "${BRANCH}" != "main" ]; then
            echo "‚ùå Non-staging deploys must be from main (branch=${BRANCH})" >&2
            exit 1
          fi

          DIGEST=$(aws ecr describe-images \
            --repository-name afu9/control-center \
            --image-ids imageTag="${IMAGE_TAG}" \
            --region "${{ env.AWS_REGION }}" \
            --query 'imageDetails[0].imageDigest' \
            --output text)

          if [ -z "${DIGEST:-}" ] || [ "${DIGEST}" = "None" ]; then
            echo "‚ùå Could not resolve ECR digest for ${IMAGE_TAG}" >&2
            exit 1
          fi

          echo "digest=${DIGEST}" >> "$GITHUB_OUTPUT"

          {
            echo "## Deploy Provenance"
            echo "- Branch: \`${BRANCH}\`"
            echo "- SHA: \`${SHA}\`"
            echo "- Environment: \`${DEPLOY_ENV}\`"
            echo "- Control Center Image: \`${REPO_URI}:${IMAGE_TAG}\`"
            echo "- Control Center Digest: \`${DIGEST}\`"
            echo "- ECS Cluster: \`${CLUSTER}\`"
            echo "- ECS Service: \`${SERVICE}\`"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: "Guardrail: block legacy DB secret (/master)"
        shell: bash
        run: |
          set -euo pipefail
          node scripts/guard-legacy-db-secret.js

      - name: Create new task definition with environment tags
        id: task-def
        shell: bash
        env:
          DEPLOY_EVENTS_TOKEN: ${{ secrets.DEPLOY_EVENTS_TOKEN }}
          SHOULD_MIGRATE: ${{ steps.gates.outputs.should_migrate }}
        run: |
          set -euo pipefail
          echo "Creating new task definition for ${{ steps.target.outputs.env_label }} with env-tagged images..."
          echo "should_migrate=${SHOULD_MIGRATE}"

          SMOKE_KEY_SECRET_ID=""
          SMOKE_KEY_VALUE_FROM=""
          if [ "${{ steps.target.outputs.deploy_env }}" = "staging" ]; then
            # Canonical identifier (must appear in this workflow for repo invariants).
            # Note: ECS `valueFrom` treats non-ARN strings as SSM Parameter Store names.
            # To inject from Secrets Manager, we resolve the secret ARN by name at deploy time.
            SMOKE_KEY_SECRET_ID="afu9/stage/smoke-key"
            echo "Using stage smoke key secret id: ${SMOKE_KEY_SECRET_ID}"

            # Prefer Secrets Manager (resolved by name, no hardcoded suffix ARN in repo).
            set +e
            SMOKE_KEY_VALUE_FROM=$(aws secretsmanager describe-secret \
              --secret-id "${SMOKE_KEY_SECRET_ID}" \
              --region "${{ env.AWS_REGION }}" \
              --query 'ARN' \
              --output text 2>smoke-key.describe.err)
            SMOKE_KEY_DESCRIBE_RC=$?
            set -e

            if [ "${SMOKE_KEY_DESCRIBE_RC}" -ne 0 ]; then
              echo "‚ùå Failed to resolve Secrets Manager secret ARN by name: ${SMOKE_KEY_SECRET_ID}" >&2
              echo "   This deploy must NOT fall back to SSM (it causes ECS to treat the identifier as an SSM parameter name)." >&2
              echo "   AWS CLI error:" >&2
              cat smoke-key.describe.err >&2 || true
              echo "   Remedy:" >&2
              echo "   - Ensure the secret exists in Secrets Manager (name: ${SMOKE_KEY_SECRET_ID}) in region ${{ env.AWS_REGION }}" >&2
              echo "   - Ensure the deploy role can call secretsmanager:DescribeSecret" >&2
              exit 1
            fi

            if [ -z "${SMOKE_KEY_VALUE_FROM:-}" ] || [ "${SMOKE_KEY_VALUE_FROM}" = "None" ]; then
              echo "‚ùå Secrets Manager secret not resolvable by name (${SMOKE_KEY_SECRET_ID})." >&2
              echo "   Expected a secret named exactly: ${SMOKE_KEY_SECRET_ID} (region ${{ env.AWS_REGION }})." >&2
              exit 1
            fi

            if ! echo "${SMOKE_KEY_VALUE_FROM}" | grep -q '^arn:aws:secretsmanager:'; then
              echo "‚ùå Resolved smoke key valueFrom is not a Secrets Manager ARN: ${SMOKE_KEY_VALUE_FROM}" >&2
              exit 1
            fi

            echo "Using Secrets Manager ARN for smoke key (resolved by name): ${SMOKE_KEY_VALUE_FROM}"
          fi

          if [ -z "${DEPLOY_EVENTS_TOKEN:-}" ]; then
            echo "‚ùå DEPLOY_EVENTS_TOKEN secret is missing" >&2
            exit 1
          fi
          export DEPLOY_EVENTS_TOKEN

          echo "Listing clusters for debugging..."
          aws ecs list-clusters --region "${{ env.AWS_REGION }}" --output text || true
          echo "Using cluster=${{ steps.target.outputs.ecs_cluster }} service=${{ steps.target.outputs.ecs_service }}"

          SERVICE_JSON=$(aws ecs describe-services \
            --cluster "${{ steps.target.outputs.ecs_cluster }}" \
            --services "${{ steps.target.outputs.ecs_service }}" \
            --region "${{ env.AWS_REGION }}" \
            --output json)

          if echo "$SERVICE_JSON" | jq -e '.failures | length > 0' >/dev/null; then
            echo "‚ùå ECS describe-services reported failures:" >&2
            echo "$SERVICE_JSON" | jq '.failures' >&2
            exit 1
          fi

          if [ "$(echo "$SERVICE_JSON" | jq -r '.services | length')" = "0" ]; then
            echo "‚ùå ECS service ${{ steps.target.outputs.ecs_service }} not found in cluster ${{ steps.target.outputs.ecs_cluster }}" >&2
            echo "Listing services for debugging:" >&2
            aws ecs list-services --cluster "${{ steps.target.outputs.ecs_cluster }}" --region "${{ env.AWS_REGION }}" --output json >&2
            exit 1
          fi

          TASK_DEF_ARN=$(echo "$SERVICE_JSON" | jq -r '.services[0].taskDefinition // ""')

          if [ -z "$TASK_DEF_ARN" ] || [ "$TASK_DEF_ARN" = "None" ]; then
            echo "‚ùå Task definition is empty for service ${{ steps.target.outputs.ecs_service }}" >&2
            echo "$SERVICE_JSON" | jq '.services[0]' >&2
            exit 1
          fi

          echo "Current task definition: $TASK_DEF_ARN"

          aws ecs describe-task-definition \
            --task-definition "$TASK_DEF_ARN" \
            --region "${{ env.AWS_REGION }}" \
            --query 'taskDefinition' > task-def.json

          ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)
          if [ -z "${ACCOUNT_ID:-}" ] || [ "${ACCOUNT_ID}" = "None" ]; then
            echo "‚ùå Could not resolve AWS account id" >&2
            exit 1
          fi
          export ACCOUNT_ID

          # Staging preflight (best-effort): ensure the task execution role can read the smoke key.
          # Note: GitHub deploy role may not be allowed to call iam:GetRole, so we derive executionRoleArn
          # from the task definition we just fetched.
          if [ "${{ steps.target.outputs.deploy_env }}" = "staging" ]; then
            EXEC_ROLE_ARN=$(jq -r '.executionRoleArn // empty' task-def.json)
            if [ -z "${EXEC_ROLE_ARN:-}" ]; then
              echo "‚ö†Ô∏è  Could not resolve executionRoleArn from task definition; skipping smoke-key preflight" >&2
            else
              # Secrets Manager ARNs include a rotated suffix; simulate against the wildcard.
              SMOKE_KEY_ARN_WILDCARD="arn:aws:secretsmanager:${{ env.AWS_REGION }}:${ACCOUNT_ID}:secret:afu9/stage/smoke-key-*"

              SIM_RESULT=$(aws iam simulate-principal-policy \
                --policy-source-arn "${EXEC_ROLE_ARN}" \
                --action-names "secretsmanager:GetSecretValue" "secretsmanager:DescribeSecret" \
                --resource-arns "${SMOKE_KEY_ARN_WILDCARD}" \
                --query 'EvaluationResults[].EvalDecision' \
                --output text 2>/dev/null || true)

              if [ -z "${SIM_RESULT:-}" ]; then
                echo "‚ö†Ô∏è  IAM simulation not permitted; skipping smoke-key preflight" >&2
              elif echo "${SIM_RESULT}" | grep -qiE "implicitDeny|explicitDeny"; then
                echo "‚ùå ECS task execution role cannot read staging smoke key secret: ${SMOKE_KEY_ARN_WILDCARD}" >&2
                echo "   Required: secretsmanager:GetSecretValue + secretsmanager:DescribeSecret on arn:...:secret:afu9/stage/smoke-key-*" >&2
                echo "   Remedy: deploy the CDK stack change that grants read to the execution role (staging-only)." >&2
                exit 1
              fi
            fi
          fi

          # Canonical DB secret reference for ECS Secrets Manager JSON-key selectors.
          # IMPORTANT: Using plain secret name with ":jsonKey::" can be parsed by ECS as an SSM parameter name.
          # Use an ARN prefix (rotation-safe/partial ARN) to force Secrets Manager interpretation.
          DB_SECRET_ARN="arn:aws:secretsmanager:${{ env.AWS_REGION }}:${ACCOUNT_ID}:secret:afu9/database"
          echo "Using DB secret arn prefix: $DB_SECRET_ARN"
          export DB_SECRET_ARN

          # Sanitize DB secret references before further mutation:
          # - replace legacy /master secret refs
          # - replace any fixed/rotated suffix refs with canonical ARN
          jq 'walk(
            if type=="string" then
              gsub("arn:aws:secretsmanager:[^:]+:[0-9]+:secret:afu9/database/master[^:]*:"; (env.DB_SECRET_ARN + ":"))
              | gsub("arn:aws:secretsmanager:[^:]+:[0-9]+:secret:afu9/database-[^:]+:"; (env.DB_SECRET_ARN + ":"))
              | gsub("afu9/database/master"; "afu9/database")
            else . end
          )' \
            task-def.json > task-def.sanitized.json
          mv task-def.sanitized.json task-def.json

          jq \
            --arg cc_image "${{ steps.ecr-uris.outputs.control_center }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.full_sha }}" \
            --arg mcp_github_image "${{ steps.ecr-uris.outputs.mcp_github }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.full_sha }}" \
            --arg mcp_deploy_image "${{ steps.ecr-uris.outputs.mcp_deploy }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.full_sha }}" \
            --arg mcp_observability_image "${{ steps.ecr-uris.outputs.mcp_observability }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.full_sha }}" \
            --arg mcp_runner_image "${{ steps.ecr-uris.outputs.mcp_runner }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.full_sha }}" \
             '.containerDefinitions |= map(
               if (
                 .name == "control-center" or
                 ((.image // "") | contains("/afu9/control-center:"))
               ) then .image = $cc_image
                 | .environment = ((.environment // []) | map(select(.name != "DEPLOY_EVENTS_TOKEN")) + [{name: "DEPLOY_EVENTS_TOKEN", value: env.DEPLOY_EVENTS_TOKEN}])
               elif (
                 .name == "mcp-github" or
                 ((.image // "") | contains("/afu9/mcp-github:"))
               ) then .image = $mcp_github_image
               elif (
                 .name == "mcp-deploy" or
                 ((.image // "") | contains("/afu9/mcp-deploy:"))
               ) then .image = $mcp_deploy_image
               elif (
                 .name == "mcp-observability" or
                 ((.image // "") | contains("/afu9/mcp-observability:"))
               ) then .image = $mcp_observability_image
               elif (
                 .name == "mcp-runner" or
                 ((.image // "") | contains("/afu9/mcp-runner:"))
               ) then .image = $mcp_runner_image
               else . end
                 | if .secrets then .secrets |= map(
                     if .name == "DATABASE_HOST" then
                       .valueFrom = (env.DB_SECRET_ARN + ":host::")
                     elif .name == "DATABASE_PORT" then
                       .valueFrom = (env.DB_SECRET_ARN + ":port::")
                     elif .name == "DATABASE_NAME" then
                       .valueFrom = (env.DB_SECRET_ARN + ":database::")
                     elif .name == "DATABASE_USER" then
                       .valueFrom = (env.DB_SECRET_ARN + ":username::")
                     elif .name == "DATABASE_PASSWORD" then
                       .valueFrom = (env.DB_SECRET_ARN + ":password::")
                     else . end
                   ) else . end
             ) |
             del(.taskDefinitionArn, .revision, .status, .requiresAttributes, .compatibilities, .registeredAt, .registeredBy, .deregisteredAt)' \
             task-def.json > new-task-def.json

          # Ensure runner sidecar is present for staging (optional sidecar; do not break tasks if it is down).
          if [ "${{ steps.target.outputs.deploy_env }}" = "staging" ]; then
            export MCP_RUNNER_IMAGE="${{ steps.ecr-uris.outputs.mcp_runner }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.full_sha }}"

            jq '
              def has_runner: any(.containerDefinitions[]?; .name == "mcp-runner");
              if has_runner then
                .
              else
                .containerDefinitions += [{
                  name: "mcp-runner",
                  image: env.MCP_RUNNER_IMAGE,
                  essential: false,
                  command: ["sh","-lc","set -e; if [ ! -f /app/base/src/server.js ] && [ -f /app/dist/base/src/server.js ]; then mkdir -p /app/base/src; cp /app/dist/base/src/* /app/base/src/; fi; if [ -f /app/dist/index.js ]; then exec node /app/dist/index.js; elif [ -f /app/dist/src/index.js ]; then exec node /app/dist/src/index.js; elif [ -f /app/dist/afu9-runner/src/index.js ]; then exec node /app/dist/afu9-runner/src/index.js; else echo \"No runner entrypoint found\" >&2; find /app/dist -maxdepth 4 -type f -name index.js >&2 || true; exit 1; fi"],
                  environment: [
                    { name: "DEPLOY_ENV", value: "staging" },
                    { name: "PORT", value: "3004" }
                  ],
                  portMappings: [{ containerPort: 3004, protocol: "tcp" }]
                }]
              end
            ' new-task-def.json > new-task-def.tmp.json
            mv new-task-def.tmp.json new-task-def.json

            # Ensure the runner uses the correct entrypoint even if it was already present.
            # Also materialize @afu9/mcp-base at /app/base to avoid broken file: symlink installs.
            jq '
              .containerDefinitions |= map(
                if .name == "mcp-runner" then
                  .command = ["sh","-lc","set -e; if [ ! -f /app/base/src/server.js ] && [ -f /app/dist/base/src/server.js ]; then mkdir -p /app/base/src; cp /app/dist/base/src/* /app/base/src/; fi; if [ -f /app/dist/index.js ]; then exec node /app/dist/index.js; elif [ -f /app/dist/src/index.js ]; then exec node /app/dist/src/index.js; elif [ -f /app/dist/afu9-runner/src/index.js ]; then exec node /app/dist/afu9-runner/src/index.js; else echo \"No runner entrypoint found\" >&2; find /app/dist -maxdepth 4 -type f -name index.js >&2 || true; exit 1; fi"]
                else . end
              )
            ' new-task-def.json > new-task-def.tmp.json
            mv new-task-def.tmp.json new-task-def.json

            jq '
              .containerDefinitions |= map(
                if .name == "control-center" then
                  .environment = ((.environment // [])
                    | map(select(.name != "MCP_RUNNER_URL" and .name != "MCP_RUNNER_ENDPOINT"))
                    + [
                      {name: "MCP_RUNNER_URL", value: "http://localhost:3004"},
                      {name: "MCP_RUNNER_ENDPOINT", value: "http://localhost:3004"}
                    ])
                else . end
              )
            ' new-task-def.json > new-task-def.tmp.json
            mv new-task-def.tmp.json new-task-def.json
          fi

          # Ensure stage smoke auth bypass key is always injected for staging service task definitions.
          # This workflow registers a task definition directly, so CDK-only injection is not sufficient.
          if [ "${{ steps.target.outputs.deploy_env }}" = "staging" ]; then
            jq --arg smoke_value_from "${SMOKE_KEY_VALUE_FROM}" '
              .containerDefinitions |= map(
                if .name == "control-center" then
                  .secrets = ((.secrets // []) | map(select(.name != "AFU9_SMOKE_KEY")) + [{name:"AFU9_SMOKE_KEY", valueFrom:$smoke_value_from}])
                else . end
              )
            ' new-task-def.json > new-task-def.tmp.json
            mv new-task-def.tmp.json new-task-def.json

            INJECTED_SMOKE_VALUE_FROM=$(jq -r '.containerDefinitions[] | select(.name=="control-center") | .secrets[]? | select(.name=="AFU9_SMOKE_KEY") | .valueFrom' new-task-def.json | head -n 1)
            echo "Injected AFU9_SMOKE_KEY valueFrom: ${INJECTED_SMOKE_VALUE_FROM}"
            if [ -z "${INJECTED_SMOKE_VALUE_FROM:-}" ] || ! echo "${INJECTED_SMOKE_VALUE_FROM}" | grep -q '^arn:aws:secretsmanager:'; then
              echo "‚ùå AFU9_SMOKE_KEY injection is not a Secrets Manager ARN; refusing to deploy." >&2
              exit 1
            fi

            if ! jq -e '.containerDefinitions[] | select(.name=="control-center") | .secrets[]? | select(.name=="AFU9_SMOKE_KEY")' new-task-def.json >/dev/null; then
              echo "‚ùå AFU9_SMOKE_KEY not injected for staging task definition" >&2
              exit 1
            fi
          else
            # Safety: prod must never receive the stage smoke key.
            jq '
              .containerDefinitions |= map(
                if .name == "control-center" then
                  .secrets = ((.secrets // []) | map(select(.name != "AFU9_SMOKE_KEY")))
                else . end
              )
            ' new-task-def.json > new-task-def.tmp.json
            mv new-task-def.tmp.json new-task-def.json

            if jq -e '.containerDefinitions[] | select(.name=="control-center") | .secrets[]? | select(.name=="AFU9_SMOKE_KEY")' new-task-def.json >/dev/null; then
              echo "‚ùå AFU9_SMOKE_KEY present in production task definition" >&2
              exit 1
            fi
          fi

          if jq -re '.. | select(type=="string" and test("/master"))' new-task-def.json >/dev/null; then
            echo "‚ùå Detected legacy '/master' reference in task definition; aborting." >&2
            jq -r '.. | select(type=="string" and test("/master"))' new-task-def.json >&2
            exit 1
          fi

          if jq -re '.containerDefinitions[] | select(.name=="control-center") | .secrets[]? | select(.name|test("^DATABASE_(HOST|PORT|NAME|USER|PASSWORD)$")) | select((.valueFrom|startswith(env.DB_SECRET_ARN + ":"))|not)' new-task-def.json >/dev/null; then
            echo "‚ùå DATABASE_* secrets are not using canonical DB secret ARN; aborting." >&2
            jq -r '.containerDefinitions[] | select(.name=="control-center") | .secrets[]? | select(.name|test("^DATABASE_(HOST|PORT|NAME|USER|PASSWORD)$")) | "\(.name)=\(.valueFrom)"' new-task-def.json >&2
            exit 1
          fi

          TAG_PREFIX="${{ steps.target.outputs.tag_prefix }}"
          if [ "${TAG_PREFIX}" = "prod" ]; then
            if jq -re '.containerDefinitions[]? | select((.image // "") | test(":stage-"))' new-task-def.json >/dev/null; then
              echo "‚ùå Task definition still references stage images during prod deploy; aborting." >&2
              jq -r '.containerDefinitions[]? | select((.image // "") | test(":stage-")) | "\(.name): \(.image)"' new-task-def.json >&2
              exit 1
            fi
          elif [ "${TAG_PREFIX}" = "stage" ]; then
            if jq -re '.containerDefinitions[]? | select((.image // "") | test(":prod-"))' new-task-def.json >/dev/null; then
              echo "‚ùå Task definition still references prod images during stage deploy; aborting." >&2
              jq -r '.containerDefinitions[]? | select((.image // "") | test(":prod-")) | "\(.name): \(.image)"' new-task-def.json >&2
              exit 1
            fi
          else
            echo "‚ùå Unexpected tag prefix: ${TAG_PREFIX}" >&2
            exit 1
          fi

          NEW_TASK_DEF_ARN=$(aws ecs register-task-definition \
            --cli-input-json file://new-task-def.json \
            --region "${{ env.AWS_REGION }}" \
            --query 'taskDefinition.taskDefinitionArn' \
            --output text)

          MIGRATION_TASK_DEF_ARN=""
          if [ "${SHOULD_MIGRATE}" = "true" ]; then
            echo "Preparing migration task definition (should_migrate=true)"

            # Migration tasks must not depend on sidecar images.
            # ECS pulls all images in a task definition, even if we only override the command
            # for one container. If the service task def includes mcp-* sidecars (stage-latest),
            # migrations would fail before the app container runs.
            jq '.containerDefinitions |= map(select(.name=="control-center"))
                | .containerDefinitions[0] |= del(.dependsOn)
                ' new-task-def.json > migration-task-def.json

            MIGRATION_TASK_DEF_ARN=$(aws ecs register-task-definition \
              --cli-input-json file://migration-task-def.json \
              --region "${{ env.AWS_REGION }}" \
              --query 'taskDefinition.taskDefinitionArn' \
              --output text)
          else
            echo "Skipping migration task definition (should_migrate=false)"
          fi

          echo "new_task_def_arn=$NEW_TASK_DEF_ARN" >> "$GITHUB_OUTPUT"
          echo "migration_task_def_arn=$MIGRATION_TASK_DEF_ARN" >> "$GITHUB_OUTPUT"
          echo "New task definition: $NEW_TASK_DEF_ARN"
          if [ -n "${MIGRATION_TASK_DEF_ARN}" ]; then
            echo "Migration task definition: $MIGRATION_TASK_DEF_ARN"
          fi

      - name: Run database migrations (gate)
        id: migrate
        if: steps.gates.outputs.should_migrate == 'true'
        shell: bash
        run: |
          set -euo pipefail
          echo "üèóÔ∏è Running database migrations for ${{ steps.target.outputs.env_label }}..."

          # Reuse the service's awsvpc network configuration
          aws ecs describe-services \
            --cluster "${{ steps.target.outputs.ecs_cluster }}" \
            --services "${{ steps.target.outputs.ecs_service }}" \
            --region "${{ env.AWS_REGION }}" \
            --query 'services[0].networkConfiguration.awsvpcConfiguration' \
            --output json > awsvpc.json

          SUBNETS=$(jq -r '.subnets | join(",")' awsvpc.json)
          SGS=$(jq -r '.securityGroups | join(",")' awsvpc.json)
          ASSIGN_PUBLIC_IP=$(jq -r '.assignPublicIp // "DISABLED"' awsvpc.json)

          if [ -z "${SUBNETS:-}" ] || [ -z "${SGS:-}" ]; then
            echo "‚ùå Could not determine VPC networking from ECS service" >&2
            cat awsvpc.json >&2 || true
            exit 1
          fi

          # Run a one-off Fargate task using the NEW task definition and override the command
          OVERRIDES_JSON=$(jq -nc '{
            containerOverrides: [{
              name: "control-center",
              command: ["bash","-lc","bash ./scripts/db-migrate.sh"],
              environment: []
            }]
          }')

          TASK_ARN=$(aws ecs run-task \
            --cluster "${{ steps.target.outputs.ecs_cluster }}" \
            --launch-type FARGATE \
            --task-definition "${{ steps.task-def.outputs.migration_task_def_arn }}" \
            --network-configuration "awsvpcConfiguration={subnets=[$SUBNETS],securityGroups=[$SGS],assignPublicIp=$ASSIGN_PUBLIC_IP}" \
            --overrides "$OVERRIDES_JSON" \
            --region "${{ env.AWS_REGION }}" \
            --query 'tasks[0].taskArn' \
            --output text)

          if [ -z "${TASK_ARN:-}" ] || [ "$TASK_ARN" = "None" ]; then
            echo "‚ùå Failed to start migration task" >&2
            exit 1
          fi

          echo "Migration task: $TASK_ARN"
          echo "task_arn=$TASK_ARN" >> "$GITHUB_OUTPUT"

          aws ecs wait tasks-stopped \
            --cluster "${{ steps.target.outputs.ecs_cluster }}" \
            --tasks "$TASK_ARN" \
            --region "${{ env.AWS_REGION }}"

          aws ecs describe-tasks \
            --cluster "${{ steps.target.outputs.ecs_cluster }}" \
            --tasks "$TASK_ARN" \
            --region "${{ env.AWS_REGION }}" \
            --output json > migration-task.json

          echo "Stopped reason:"
          jq -r '.tasks[0].stoppedReason // "(none)"' migration-task.json

          echo "Container statuses (name / lastStatus / exitCode / reason):"
          jq -r '.tasks[0].containers[] | "- \(.name): status=\(.lastStatus) exitCode=\(.exitCode // "null") reason=\(.reason // "(none)")"' migration-task.json

          EXIT_CODE=$(jq -r '
            .tasks[0].containers
            | (map(select(.name=="control-center").exitCode)[0] // .[0].exitCode // empty)
          ' migration-task.json)

          if [ -z "${EXIT_CODE:-}" ] || [ "$EXIT_CODE" = "null" ]; then
            echo "‚ùå Could not determine migration task exit code (task=$TASK_ARN)" >&2
            exit 1
          fi

          echo "Migration exit code: $EXIT_CODE"
          if [ "$EXIT_CODE" != "0" ]; then
            echo "‚ùå Database migrations failed" >&2
            exit 1
          fi

          echo "‚úÖ Database migrations applied"

      - name: Check deployment verdict gate
        env:
          DEPLOYMENT_VERDICT: ${{ vars.DEPLOYMENT_VERDICT }}
        shell: bash
        run: |
          set -euo pipefail
          echo "üîç Checking deployment verdict gate..."
          echo "Issue B3: No deployment without GREEN verdict"
          echo ""
          VERDICT="${DEPLOYMENT_VERDICT:-GREEN}"
          node scripts/check-deployment-gate.js "$VERDICT"
          echo ""
          echo "‚úÖ Verdict gate check passed - proceeding with deployment"

      - name: Update ECS service
        shell: bash
        run: |
          set -euo pipefail
          echo "üöÄ Updating ECS service with new task definition..."

          # Store cluster and service names for cleaner code
          CLUSTER="${{ steps.target.outputs.ecs_cluster }}"
          SERVICE="${{ steps.target.outputs.ecs_service }}"
          TASK_DEF="${{ steps.task-def.outputs.new_task_def_arn }}"
          REGION="${{ env.AWS_REGION }}"

          echo "Cluster: $CLUSTER"
          echo "Service: $SERVICE"
          echo "Task Definition: $TASK_DEF"

          # Check for multiple task sets before update (common failure cause)
          echo "Checking for existing task sets..."
          SERVICE_INFO=$(aws ecs describe-services \
            --cluster "$CLUSTER" \
            --services "$SERVICE" \
            --region "$REGION" \
            --output json)

          # Verify service exists
          SERVICE_COUNT=$(echo "$SERVICE_INFO" | jq -r '.services | length')
          if [ "$SERVICE_COUNT" -eq 0 ]; then
            echo "‚ùå Service $SERVICE not found in cluster $CLUSTER"
            exit 1
          fi

          # Safe to access .services[0] since we verified SERVICE_COUNT > 0
          TASK_SET_COUNT=$(echo "$SERVICE_INFO" | jq -r '.services[0].taskSets // [] | length')
          echo "Task sets found: $TASK_SET_COUNT"

          if [ "$TASK_SET_COUNT" -gt 1 ]; then
            echo "‚ùå Multiple task sets detected. This can cause update failures."
            echo "$SERVICE_INFO" | jq -r '.services[0].taskSets'
            echo "To resolve: Wait for deployments to complete or manually clean up task sets."
            exit 1
          fi

          # Capture current service state for diagnostics
          echo "Current service state:"
          echo "$SERVICE_INFO" | jq -r '.services[0] | {status,runningCount,desiredCount,deploymentController}'

          # Check if service is INACTIVE
          SERVICE_STATUS=$(echo "$SERVICE_INFO" | jq -r '.services[0].status // "UNKNOWN"')
          if [ "$SERVICE_STATUS" = "INACTIVE" ]; then
            echo "‚ùå Service $SERVICE is INACTIVE and cannot be updated"
            echo "The service must be in ACTIVE status to receive updates"
            echo "Check AWS Console or run: aws ecs describe-services --cluster $CLUSTER --services $SERVICE --region $REGION"
            exit 1
          fi
          if [ "$SERVICE_STATUS" = "UNKNOWN" ] || [ -z "$SERVICE_STATUS" ]; then
            echo "‚ö†Ô∏è  Warning: Could not determine service status (got: '$SERVICE_STATUS')"
            echo "Proceeding with update, but this may fail if service is not ACTIVE"
          fi

          # Perform the update with detailed error capture
          echo "Executing service update..."
          if ! UPDATE_OUTPUT=$(aws ecs update-service \
            --cluster "$CLUSTER" \
            --service "$SERVICE" \
            --task-definition "$TASK_DEF" \
            --region "$REGION" \
            --output json 2>&1); then
            echo "‚ùå ECS service update failed with error:"
            echo "$UPDATE_OUTPUT"
            exit 1
          fi

          echo "‚úÖ Service update initiated successfully"
          # Safe extraction of deployment info with try operator
          echo "$UPDATE_OUTPUT" | jq -r 'try (.service.deployments[] | "Deployment: \(.id) - Status: \(.status) - Desired: \(.desiredCount)") // "No deployment information available"'

      - name: Wait for ECS service stable (with diagnostics)
        shell: bash
        run: |
          set -euo pipefail

          CLUSTER="${{ steps.target.outputs.ecs_cluster }}"
          SERVICE="${{ steps.target.outputs.ecs_service }}"
          EXPECTED_TASK_DEF="${{ steps.task-def.outputs.new_task_def_arn }}"
          REGION="${{ env.AWS_REGION }}"

          # The default AWS waiter can fail with exit code 255 without useful context.
          # Poll the service state and dump events/task stop reasons on timeout.
          TIMEOUT_SECONDS=1200
          INTERVAL_SECONDS=15
          START_TS=$(date +%s)

          echo "Waiting for ECS service to become stable..."
          echo "Cluster: ${CLUSTER}"
          echo "Service: ${SERVICE}"
          echo "Region:  ${REGION}"
          echo "Expected task definition: ${EXPECTED_TASK_DEF}"
          echo "Timeout: ${TIMEOUT_SECONDS}s (interval=${INTERVAL_SECONDS}s)"

          while true; do
            NOW_TS=$(date +%s)
            ELAPSED=$((NOW_TS - START_TS))

            SERVICE_JSON=$(aws ecs describe-services \
              --cluster "${CLUSTER}" \
              --services "${SERVICE}" \
              --region "${REGION}" \
              --output json)

            if echo "$SERVICE_JSON" | jq -e '.failures | length > 0' >/dev/null; then
              echo "‚ùå ECS describe-services reported failures:" >&2
              echo "$SERVICE_JSON" | jq '.failures' >&2
              exit 1
            fi

            SERVICE_COUNT=$(echo "$SERVICE_JSON" | jq -r '.services | length')
            if [ "${SERVICE_COUNT}" = "0" ]; then
              echo "‚ùå ECS service not found: ${SERVICE} (cluster=${CLUSTER})" >&2
              exit 1
            fi

            RUNNING=$(echo "$SERVICE_JSON" | jq -r '.services[0].runningCount // 0')
            DESIRED=$(echo "$SERVICE_JSON" | jq -r '.services[0].desiredCount // 0')
            STATUS=$(echo "$SERVICE_JSON" | jq -r '.services[0].status // "UNKNOWN"')
            CURRENT_TASK_DEF=$(echo "$SERVICE_JSON" | jq -r '.services[0].taskDefinition // ""')
            PRIMARY_ROLLOUT=$(echo "$SERVICE_JSON" | jq -r '(.services[0].deployments // []) | map(select(.status=="PRIMARY"))[0].rolloutState // "UNKNOWN"')
            PRIMARY_REASON=$(echo "$SERVICE_JSON" | jq -r '(.services[0].deployments // []) | map(select(.status=="PRIMARY"))[0].rolloutStateReason // ""')

            echo "[${ELAPSED}s] status=${STATUS} running=${RUNNING} desired=${DESIRED} primaryRollout=${PRIMARY_ROLLOUT} taskDef=${CURRENT_TASK_DEF} ${PRIMARY_REASON}"

            # Fail fast if ECS has stabilized on a different task definition than we requested.
            # This usually indicates circuit-breaker rollback or an update that was ignored.
            if [ -n "${EXPECTED_TASK_DEF}" ] && [ -n "${CURRENT_TASK_DEF}" ] && [ "${CURRENT_TASK_DEF}" != "${EXPECTED_TASK_DEF}" ]; then
              echo "‚ùå ECS service is using a different task definition than expected (rollback/partial deploy suspected)." >&2
              echo "   expected=${EXPECTED_TASK_DEF}" >&2
              echo "   current=${CURRENT_TASK_DEF}" >&2

              echo "\n=== Deployments ===" >&2
              echo "$SERVICE_JSON" | jq -r '.services[0].deployments' >&2 || true

              echo "\n=== Recent service events (latest 30) ===" >&2
              echo "$SERVICE_JSON" | jq -r '.services[0].events[0:30][]? | "- [\(.createdAt)] \(.message)"' >&2 || true

              echo "\n=== Stopped tasks (up to 10) ===" >&2
              STOPPED=$(aws ecs list-tasks \
                --cluster "${CLUSTER}" \
                --service-name "${SERVICE}" \
                --desired-status STOPPED \
                --max-results 10 \
                --region "${REGION}" \
                --output json || true)
              echo "${STOPPED}" | jq '.' >&2 || true

              TASK_ARNS=$(echo "${STOPPED}" | jq -r '.taskArns[]?')
              if [ -n "${TASK_ARNS}" ]; then
                echo "\n=== Stopped task details (stop reasons) ===" >&2
                aws ecs describe-tasks \
                  --cluster "${CLUSTER}" \
                  --tasks ${TASK_ARNS} \
                  --region "${REGION}" \
                  --output json | jq -r '.tasks[] | "task=\(.taskArn) stoppedReason=\(.stoppedReason // "")\n" + (.containers[] | "  - \(.name): exitCode=\(.exitCode // "null") reason=\(.reason // "")")' >&2 || true
              fi

              exit 1
            fi

            if [ "${STATUS}" = "ACTIVE" ] && [ "${RUNNING}" = "${DESIRED}" ] && [ "${PRIMARY_ROLLOUT}" = "COMPLETED" ]; then
              if [ -n "${EXPECTED_TASK_DEF}" ] && [ -n "${CURRENT_TASK_DEF}" ] && [ "${CURRENT_TASK_DEF}" != "${EXPECTED_TASK_DEF}" ]; then
                echo "‚ùå Service is stable but not on the expected task definition" >&2
                echo "   expected=${EXPECTED_TASK_DEF}" >&2
                echo "   current=${CURRENT_TASK_DEF}" >&2
                exit 1
              fi

              echo "‚úÖ Service appears stable on expected task definition"
              break
            fi

            if [ "${ELAPSED}" -ge "${TIMEOUT_SECONDS}" ]; then
              echo "‚ùå Timed out waiting for ECS service to stabilize" >&2

              echo "\n=== Service summary ===" >&2
              echo "$SERVICE_JSON" | jq -r '.services[0] | {status, desiredCount, runningCount, pendingCount, deploymentController, deployments}' >&2

              echo "\n=== Recent service events (latest 20) ===" >&2
              echo "$SERVICE_JSON" | jq -r '.services[0].events[0:20][]? | "- [\(.createdAt)] \(.message)"' >&2 || true

              echo "\n=== Running tasks (up to 10) ===" >&2
              aws ecs list-tasks \
                --cluster "${CLUSTER}" \
                --service-name "${SERVICE}" \
                --desired-status RUNNING \
                --max-results 10 \
                --region "${REGION}" \
                --output json >&2 || true

              echo "\n=== Stopped tasks (up to 10) ===" >&2
              STOPPED=$(aws ecs list-tasks \
                --cluster "${CLUSTER}" \
                --service-name "${SERVICE}" \
                --desired-status STOPPED \
                --max-results 10 \
                --region "${REGION}" \
                --output json || true)
              echo "${STOPPED}" | jq '.' >&2 || true

              TASK_ARNS=$(echo "${STOPPED}" | jq -r '.taskArns[]?')
              if [ -n "${TASK_ARNS}" ]; then
                echo "\n=== Stopped task details (stop reasons) ===" >&2
                aws ecs describe-tasks \
                  --cluster "${CLUSTER}" \
                  --tasks ${TASK_ARNS} \
                  --region "${REGION}" \
                  --output json | jq -r '.tasks[] | "task=\(.taskArn) stoppedReason=\(.stoppedReason // "")\n" + (.containers[] | "  - \(.name): exitCode=\(.exitCode // "null") reason=\(.reason // "")")' >&2 || true
              fi

              exit 1
            fi

            sleep "${INTERVAL_SECONDS}"
          done

      - name: Get service status
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          echo "Service deployment status:"
          aws ecs describe-services \
            --cluster "${{ steps.target.outputs.ecs_cluster }}" \
            --services "${{ steps.target.outputs.ecs_service }}" \
            --region "${{ env.AWS_REGION }}" \
            --query 'services[0].{status:status,runningCount:runningCount,desiredCount:desiredCount,deployments:deployments}' \
            --output table || true

      - name: Resolve ALB DNS
        id: alb-dns
        if: steps.gates.outputs.run_alb_checks == 'true'
        shell: pwsh
        run: |
          # $env:GITHUB_OUTPUT exists in GitHub Actions; locally it can be null. Write to it only when present.
          $writeOutput = $false
          if ($env:GITHUB_OUTPUT) { $writeOutput = $true }

          $cluster = "${{ steps.target.outputs.ecs_cluster }}"
          $service = "${{ steps.target.outputs.ecs_service }}"

          # 1) Describe service to get target group ARN
          $svcJson = aws ecs describe-services --cluster $cluster --services $service --region "${{ env.AWS_REGION }}" --output json
          $tgArn = ($svcJson | ConvertFrom-Json).services[0].loadBalancers[0].targetGroupArn
          if (-not $tgArn) {
            Write-Error "No loadBalancers attached to service $service in cluster $cluster"; exit 1
          }

          # 2) Describe target group to get ALB ARN
          $tgJson = aws elbv2 describe-target-groups --target-group-arns $tgArn --region "${{ env.AWS_REGION }}" --output json
          $albArn = ($tgJson | ConvertFrom-Json).TargetGroups[0].LoadBalancerArns[0]
          if (-not $albArn) {
            Write-Error "No LoadBalancerArn found for target group $tgArn"; exit 1
          }

          # 3) Describe load balancer to get DNS name
          $albJson = aws elbv2 describe-load-balancers --load-balancer-arns $albArn --region "${{ env.AWS_REGION }}" --output json
          $albDns = ($albJson | ConvertFrom-Json).LoadBalancers[0].DNSName
          if (-not $albDns) {
            Write-Error "No DNSName found for ALB $albArn"; exit 1
          }

          if ($writeOutput) {
            "alb_dns_name=$albDns" | Out-File -FilePath $env:GITHUB_OUTPUT -Encoding utf8 -Append
          } else {
            Write-Host "alb_dns_name=$albDns"
          }

          Write-Host "Resolved ALB DNS: $albDns"

      - name: ALB target health (no jq)
        if: steps.gates.outputs.run_alb_checks == 'true'
        shell: pwsh
        env:
          DEPLOY_ENV: ${{ steps.target.outputs.deploy_env }}
          AWS_REGION: ${{ env.AWS_REGION }}
          ALB_DNS_NAME: ${{ steps.alb-dns.outputs.alb_dns_name }}
        run: |
          $deployEnv = $env:DEPLOY_ENV
          if ($deployEnv -ne 'production' -and $deployEnv -ne 'staging') {
            Write-Error "Unsupported DEPLOY_ENV: $deployEnv"; exit 1
          }

          $albDns = $env:ALB_DNS_NAME
          if (-not $albDns) {
            Write-Error "ALB_DNS_NAME is empty"; exit 1
          }

          $region = $env:AWS_REGION
          if (-not $region) { $region = 'eu-central-1' }

          $envLabel = if ($deployEnv -eq 'staging') { 'stage' } else { 'prod' }

          # 1) Resolve ALB ARN from DNS name
          $albArn = (aws elbv2 describe-load-balancers `
            --region $region `
            --query "LoadBalancers[?DNSName=='$albDns'].LoadBalancerArn | [0]" `
            --output text) 2>&1

          if (-not $albArn -or $albArn -eq 'None') {
            Write-Error "Could not resolve ALB ARN for DNS '$albDns' (got: '$albArn')"; exit 1
          }

          Write-Host "ALB ARN: $albArn"
          Write-Host "Environment: $envLabel"

          # 2) Select target group ARNs (no JSON parsing)
          if ($deployEnv -eq 'staging') {
            $query = "TargetGroups[?contains(TargetGroupName, 'stage')].TargetGroupArn"
          } else {
            $query = "TargetGroups[?(!contains(TargetGroupName, 'stage'))].TargetGroupArn"
          }

          $tgArnsText = (aws elbv2 describe-target-groups `
            --load-balancer-arn $albArn `
            --region $region `
            --query $query `
            --output text) 2>&1

          if (-not $tgArnsText -or $tgArnsText -eq 'None') {
            Write-Error "No target groups matched for env='$envLabel' (query=$query). Output: $tgArnsText"; exit 1
          }

          # Output text is space-separated; keep the first TG ARN as the selected group
          $tgArn = ($tgArnsText -split "\s+" | Where-Object { $_ -and $_ -ne 'None' } | Select-Object -First 1)
          if (-not $tgArn) {
            Write-Error "Failed to select a target group ARN from output: $tgArnsText"; exit 1
          }

          Write-Host "Target group ARN: $tgArn"

          # 3) Poll target health states until all are healthy (or timeout)
          $maxAttempts = 30
          $sleepSeconds = 10
          for ($attempt = 1; $attempt -le $maxAttempts; $attempt++) {
            $statesText = (aws elbv2 describe-target-health `
              --target-group-arn $tgArn `
              --region $region `
              --query "TargetHealthDescriptions[].TargetHealth.State" `
              --output text) 2>&1

            if ($statesText -match 'AccessDenied' -or $statesText -match 'is not authorized') {
              Write-Error "AccessDenied calling DescribeTargetHealth. The deploy role needs 'elasticloadbalancing:DescribeTargetHealth'. Details: $statesText";
              exit 1
            }

            $states = @()
            if ($statesText -and $statesText -ne 'None') {
              $states = ($statesText -split "\s+" | Where-Object { $_ })
            }

            if ($states.Count -eq 0) {
              Write-Host "Attempt $attempt/${maxAttempts}: states=(no targets yet)"
            } else {
              Write-Host "Attempt $attempt/${maxAttempts}: states=$statesText"
            }

            if ($states.Count -gt 0 -and ($states | Where-Object { $_ -ne 'healthy' }).Count -eq 0) {
              Write-Host "‚úÖ All targets are healthy";
              exit 0
            }

            if ($attempt -eq $maxAttempts) {
              Write-Error "‚ùå ALB target health did not become healthy in time. Final states: $statesText";
              exit 1
            }

            Start-Sleep -Seconds $sleepSeconds
          }

      - name: Post-deploy readiness (/api/ready)
        shell: pwsh
        env:
          ALB_DNS_NAME: ${{ steps.alb-dns.outputs.alb_dns_name }}
          READY_HOST: ${{ steps.target.outputs.ready_host }}
        run: |
          $ErrorActionPreference = 'Stop'

          # Prefer probing the public host so TLS SNI/cert matches.
          $target = if ($env:READY_HOST) { $env:READY_HOST } else { $env:ALB_DNS_NAME }
          if (-not $target) { Write-Error "Readiness probe has no target (READY_HOST/ALB_DNS_NAME empty)"; exit 1 }

          $uri = "https://$target/api/ready"
          Write-Host "Probing readiness at $uri"

          $lastError = $null
          for ($i = 1; $i -le 30; $i++) {
            try {
              $resp = Invoke-WebRequest -Uri $uri -TimeoutSec 10
              if ($resp.StatusCode -eq 200) {
                Write-Host "Ready (attempt $i)"
                break
              }
              $lastError = "Non-200: $($resp.StatusCode)"
            } catch {
              $lastError = $_.Exception.Message
            }

            if ($i -eq 30) {
              Write-Error "Readiness probe failed after $i attempts. Last error: $lastError"
              exit 1
            }
            Start-Sleep -Seconds 5
          }

      - name: Post-Deploy Image Verification (E7.0.2)
        shell: bash
        env:
          DEPLOY_ENV: ${{ steps.target.outputs.deploy_env }}
          GIT_SHA: ${{ github.sha }}
          ECS_CLUSTER: ${{ steps.target.outputs.ecs_cluster }}
          ECS_SERVICE: ${{ steps.target.outputs.ecs_service }}
          AWS_REGION: ${{ env.AWS_REGION }}
        run: |
          set -euo pipefail
          echo "üîç Running Post-Deploy Image Verification (E7.0.2)"
          echo "Verifying running task definition uses only images from this deploy..."
          echo ""
          
          # Verify that the deployed task definition contains ONLY images
          # from the current deploy (no mixed/stale references)
          npx ts-node scripts/post-deploy-image-verification.ts
          
          echo ""
          echo "‚úÖ Verification passed - task definition matches deploy"

      - name: MCP Catalog Verification Gate (E7.0.3)
        shell: bash
        env:
          MCP_VERIFY_ENDPOINT: https://${{ steps.target.outputs.ready_host }}/api/mcp/verify
          MCP_VERIFY_TIMEOUT_MS: '60000'
        run: |
          set -euo pipefail
          echo "üîç Running MCP Catalog Verification Gate (E7.0.3)"
          echo "Verifying catalog matches runtime MCP server configuration..."
          echo "Endpoint: ${MCP_VERIFY_ENDPOINT}"
          echo ""
          
          # Verify that the MCP catalog is consistent with runtime configuration
          # This prevents false-green scenarios where health checks pass but
          # catalog references wrong endpoints/ports
          npx ts-node scripts/mcp-catalog-verify-gate.ts
          
          echo ""
          echo "‚úÖ MCP catalog verification passed"

      - name: Record deploy event (internal)
        shell: bash
        env:
          DEPLOY_EVENTS_TOKEN: ${{ secrets.DEPLOY_EVENTS_TOKEN }}
          READY_HOST: ${{ steps.target.outputs.ready_host }}
          DEPLOY_ENV: ${{ steps.target.outputs.deploy_env }}
          APP_VERSION: ${{ steps.build-meta.outputs.version }}
        run: |
          set -euo pipefail
          if [ -z "${DEPLOY_EVENTS_TOKEN:-}" ]; then
            echo "‚ùå DEPLOY_EVENTS_TOKEN secret is missing" >&2
            exit 1
          fi

          URL="https://${READY_HOST}/api/internal/deploy-events"
          RUN_URL="https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          MESSAGE="Deploy succeeded: env=${DEPLOY_ENV} service=control-center sha=${{ github.sha }} run=${{ github.run_id }} (${RUN_URL})"

            STATUS="success"
            VERSION="${APP_VERSION}"
            COMMIT_HASH="${{ github.sha }}"

          BODY=$(jq -nc \
            --arg env "${DEPLOY_ENV}" \
            --arg service "control-center" \
            --arg message "${MESSAGE}" \
            --arg version "${VERSION}" \
            --arg commit_hash "${COMMIT_HASH}" \
            --arg status "${STATUS}" \
            '{env:$env,service:$service,version:$version,commit_hash:$commit_hash,status:$status,message:$message}')

          HTTP_CODE=$(curl -sS -o /tmp/deploy-event.json -w "%{http_code}" \
            -X POST "$URL" \
            -H "Content-Type: application/json" \
            -H "x-internal-token: ${DEPLOY_EVENTS_TOKEN}" \
            -H "x-deploy-sha: ${{ github.sha }}" \
            --data "$BODY")

          if [ "$HTTP_CODE" -lt 200 ] || [ "$HTTP_CODE" -ge 300 ]; then
            echo "‚ùå Failed to record deploy event (HTTP $HTTP_CODE)" >&2
            cat /tmp/deploy-event.json >&2 || true
            exit 1
          fi

      - name: Deployment summary
        shell: bash
        run: |
          set -euo pipefail
          {
            echo "## Deployment Summary"
            echo ""
            echo "‚úÖ **Deployment completed successfully**"
            echo "Environment: ${{ steps.target.outputs.env_label }}"
            echo ""
            echo "### Images:"
            echo "- Control Center: \`${{ steps.ecr-uris.outputs.control_center }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.short_sha }}\`"
            echo "- MCP GitHub (built/pushed): \`${{ steps.ecr-uris.outputs.mcp_github }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.short_sha }}\`"
            echo "- MCP Deploy (built/pushed): \`${{ steps.ecr-uris.outputs.mcp_deploy }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.short_sha }}\`"
            echo "- MCP Observability (built/pushed): \`${{ steps.ecr-uris.outputs.mcp_observability }}:${{ steps.target.outputs.tag_prefix }}-${{ steps.image-tags.outputs.short_sha }}\`"
            echo ""
            echo "### ECS Configuration:"
            echo "- Cluster: \`${{ steps.target.outputs.ecs_cluster }}\`"
            echo "- Service: \`${{ steps.target.outputs.ecs_service }}\`"
            echo "- Region: \`${{ env.AWS_REGION }}\`"
            echo "- Task Definition: \`${{ steps.task-def.outputs.new_task_def_arn }}\`"
            echo "- ALB DNS: ${{ steps.alb-dns.outputs.alb_dns_name }}"
            echo ""
            echo "### Rollback Information:"
            echo "- Git SHA: \`${{ github.sha }}\`"
            echo "- Short SHA: \`${{ steps.image-tags.outputs.short_sha }}\`"
            echo "- Environment: ${{ steps.target.outputs.env_label }}"
          } >> "$GITHUB_STEP_SUMMARY"

# ============================================================================
# How to verify (read-only checks)
# - npx cdk diff Afu9EcsStack -c afu9-domain=afu-9.com -c afu9-multi-env=false -c afu9-enable-https=true -c afu9-manage-dns=false -c environment=prod
# - aws ecs list-services --cluster afu9-cluster --region eu-central-1
# - curl -s -H "Host: afu-9.com" https://<alb-dns>/api/ready
# ============================================================================
